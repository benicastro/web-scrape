{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c235ab4",
   "metadata": {},
   "source": [
    "# Web Scraping in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db341e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1e9e7",
   "metadata": {},
   "source": [
    "#### Course Description\n",
    "\n",
    "> The ability to build tools capable of retrieving and parsing information stored across the internet has been and continues to be valuable in many veins of data science. In this course, you will learn to navigate and parse html code, and build tools to crawl websites automatically. Although our scraping will be conducted using the versatile Python library scrapy, many of the techniques you learn in this course can be applied to other popular Python libraries as well, including BeautifulSoup and Selenium. Upon the completion of this course, you will have a strong mental model of html structure, will be able to build tools to parse html code and access desired information, and create a simple scrapy spiders to crawl the web at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a591c228",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fb7b6",
   "metadata": {},
   "source": [
    "## Introduction to HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f78f2",
   "metadata": {},
   "source": [
    "> Learn the structure of HTML. We begin by explaining why web scraping can be a valuable addition to your data science toolbox and then delving into some basics of HTML. We end the chapter by giving a brief introduction on XPath notation, which is used to navigate the elements within HTML code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed2eea",
   "metadata": {},
   "source": [
    "### HyperText Markup Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1a264",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfb4fb",
   "metadata": {},
   "source": [
    "### Crash Course in XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024a6fa",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9764d26f",
   "metadata": {},
   "source": [
    "## XPaths and Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9fd79",
   "metadata": {},
   "source": [
    "### Xpathology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4830e1",
   "metadata": {},
   "source": [
    "### Off the Beaten XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328f4a6",
   "metadata": {},
   "source": [
    "### Introduction to the scrapy Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca04ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35ce286",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <div class=\"hello datacamp\">\n",
    "            <p>Hello World!</p>\n",
    "        </div>\n",
    "        <p>Enjoy DataCamp!</p>\n",
    "    </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079860b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scrapy Selector object using a string with the html code\n",
    "sel = Selector(text=html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76716710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//p' data='<p>Hello World!</p>'>,\n",
       " <Selector xpath='//p' data='<p>Enjoy DataCamp!</p>'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all p\n",
    "sel.xpath(\"//p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbc0627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>Hello World!</p>', '<p>Enjoy DataCamp!</p>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.xpath(\"//p\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd511c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Hello World!</p>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.xpath(\"//p\").extract_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4bd5106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Enjoy DataCamp!</p>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = sel.xpath('//p')\n",
    "second_p = ps[1]\n",
    "second_p.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b598f9",
   "metadata": {},
   "source": [
    "### The Source of the Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc860de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea2d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "html = requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efa5936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = Selector(text=html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd32f37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Selector xpath=None data='<html class=\"client-nojs vector-featu...'>\n"
     ]
    }
   ],
   "source": [
    "print(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bee837",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47120d0b",
   "metadata": {},
   "source": [
    "## CSS Locators, Chaining, and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910bfc2",
   "metadata": {},
   "source": [
    "### From XPath to CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2e3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <div class=\"hello datacamp\">\n",
    "            <p>Hello World!</p>\n",
    "        </div>\n",
    "        <p>Enjoy DataCamp!</p>\n",
    "    </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09bb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "sel = Selector(text=html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcc6bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='descendant-or-self::div/p' data='<p>Hello World!</p>'>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.css(\"div > p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2063d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>Hello World!</p>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.css(\"div > p\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7ea0c",
   "metadata": {},
   "source": [
    "### CSS Attributes and Text Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5b6d2",
   "metadata": {},
   "source": [
    "### Respond Please"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b97ec",
   "metadata": {},
   "source": [
    "### Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.datacamp.com/courses/all'\n",
    "course_divs = response.css('div.course-block')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c9230",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c8de5",
   "metadata": {},
   "source": [
    "## Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1da2e3",
   "metadata": {},
   "source": [
    "### Your First Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d692afa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 10:18:32 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapybot)\n",
      "2023-09-04 10:18:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1q  5 Jul 2022), cryptography 37.0.1, Platform Windows-10-10.0.22621-SP0\n",
      "2023-09-04 10:18:32 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-09-04 10:18:32 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-09-04 10:18:32 [scrapy.extensions.telnet] INFO: Telnet Password: 1fa75098d0dc7446\n",
      "2023-09-04 10:18:32 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-04 10:18:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-04 10:18:33 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-04 10:18:33 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-04 10:18:33 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-04 10:18:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-04 10:18:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-04 10:18:33 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-04 10:18:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'elapsed_time_seconds': 0.003004,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 4, 2, 18, 33, 451190),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'start_time': datetime.datetime(2023, 9, 4, 2, 18, 33, 448186)}\n",
      "2023-09-04 10:18:33 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SpiderClassName(scrapy.Spider):\n",
    "    name = \"spider_name\"\n",
    "    # the code of your spider\n",
    "    ...\n",
    "\n",
    "# Initiate a CrawlerProcess\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Tell the process which spider to use\n",
    "process.crawl(SpiderClassName)\n",
    "\n",
    "# Start the crawling process\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be385e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCspider(scrapy.Spider):\n",
    "    \n",
    "    name = 'dc_spider'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        urls = ['https://www.datacamp.com/courses/all']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        # simple example: write out the html\n",
    "        html_file = \"DC_courses.html\"\n",
    "        with open(html_file, 'wb') as fout:\n",
    "            fout.write(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d875c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    pass\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9b28a",
   "metadata": {},
   "source": [
    "### Start Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f34c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_requests(self):\n",
    "        urls = ['https://www.datacamp.com/courses/all']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3824958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( \"Hello World!\" )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c813b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url=\"https://www.datacamp.com\", callback=self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90a7f2",
   "metadata": {},
   "source": [
    "### Parse and Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    # input parsing code with response that you already know!\n",
    "    # output to a file, or...\n",
    "    # crawl the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    links = response.css('div.course-block > a::attr(href)').extract()\n",
    "    filepath = 'DC_links.csv'\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.writelines([f\"{link}/n\" for link in links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624183bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = \"dcspider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = ['https://www.datacamp.com/courses/all']\n",
    "    for url in urls:\n",
    "        yield scrapy.Request( url=url, callback=self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css('div.course-block > a::attr(href)').extract()\n",
    "    for link in links:\n",
    "        yield response.follow(url=link, callback=self.parse2)\n",
    "        \n",
    "  def parse2(self, response):\n",
    "        # parse the course sites here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ed337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css('p.course-block__author-name::text').extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow(url=link, callback=self.parse_descr)\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e0f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e06f873",
   "metadata": {},
   "source": [
    "### Capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_front(self, response):\n",
    "    # Narrow in on the course blocks\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    # Direct to the course links\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    # Extract the links (as a list of strings)\n",
    "    links_to_follow = course_links.extract()\n",
    "    # Foloow the links to the next parser\n",
    "    for url in links_to_follow:\n",
    "        yield response.follow(url=url, callback=self.parse_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pages(self, response):\n",
    "    # Direct to the course title text\n",
    "    crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "    # Extract and clean the course title text\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Direct to the chapter titles text\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    # Extract and clean the chapter titles text\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[crs_title_ext] = ch_titles_ext\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b572ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88250ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css( 'p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222cbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66574782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4904eab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb1456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ba33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27db71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacfd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab74ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4753e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00200f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1006e7f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef3ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0e880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71b268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c3b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5daba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3367df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1ef17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ad6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
